{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo.bulk import create_relationships, merge_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo.matching import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"localadmin\"\n",
    "graph = Graph(uri, user=user, password=password)\n",
    "nodes = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-external",
   "metadata": {},
   "source": [
    "## Consolidating nodes via batched relationship redirection\n",
    "\n",
    "The graph APOC merge node method does everything at once so maxes out memory. Hence using some simple routines to rewire relationships in batches, until down to none, then can remove node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('consolidated_acts.pkl', 'rb') as fp:\n",
    "    consolidated_acts = pickle.load(fp)\n",
    "    print('Done so far: ', consolidated_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rels = lambda act_id: graph.run(f\"MATCH (c:Case)-[r]->(a:Act{{act_id:{act_id}}}) return count(r)\").evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batch_cases(act_id, batch_size=10):\n",
    "    query_result = graph.run(f\"\"\"\n",
    "        MATCH ()-[r]->(a:Act{{act_id:{act_id}}}) RETURN r LIMIT {batch_size}\n",
    "    \"\"\").data()\n",
    "    cases = [rel['r'].start_node.get('case_id') for rel in query_result]\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_delete_batch(to_merge_act_id, merge_into_act_id, batch_size=10):\n",
    "    print(\"Initiating batch of rewires, rels into to merge: \", count_rels(to_merge_act_id), \" and dest: \", count_rels(merge_into_act_id))\n",
    "    # get the cases\n",
    "    cases_to_rewire = extract_batch_cases(to_merge_act_id, batch_size=batch_size)\n",
    "    # wire them to the new act\n",
    "    data = [(case_id, { \"type\": \"USES_ACT\" }, merge_into_act_id) for case_id in cases_to_rewire]\n",
    "    create_relationships(graph.auto(), data, \"USES_ACT\", \n",
    "                         start_node_key=(\"Case\", \"case_id\"), \n",
    "                         end_node_key=(\"Act\", \"act_id\"))\n",
    "    # delete them from the old act\n",
    "    # print('deleting from cases: ', cases_to_rewire)\n",
    "    graph.run(f\"\"\"\n",
    "        match (c:Case)-[r]->(a:Act{{act_id:{to_merge_act_id}}}) where \n",
    "        c.case_id in ($cases) delete r;\n",
    "    \"\"\", cases=cases_to_rewire).evaluate()\n",
    "    # complete\n",
    "    left_to_merge = count_rels(to_merge_act_id)\n",
    "    at_destination = count_rels(merge_into_act_id)\n",
    "    print(\"Completed batch, rels into to merge: \", left_to_merge, \" and dest: \", at_destination)\n",
    "    return left_to_merge, at_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_nodes(to_merge_node, merge_into_node):\n",
    "    print(\"Initiating merger of nodes: \", to_merge_node, merge_into_node)\n",
    "    to_merge_act_id = int(to_merge_node.get('act_id'))\n",
    "    merge_into_act_id = int(merge_into_node.get('act_id'))\n",
    "    \n",
    "    to_merge_rels = count_rels(to_merge_act_id)\n",
    "    while to_merge_rels > 0:\n",
    "        to_merge_rels, _ = merge_and_delete_batch(to_merge_act_id, merge_into_act_id, batch_size=1000)\n",
    "    print(\"Completed copying across relationships, removing node\")\n",
    "    \n",
    "    graph.run(f\"match (a:Act{{act_id:{to_merge_act_id}}}) delete a\").evaluate()\n",
    "    print(\"Completed merger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_merge_act_id = 4942 # Criminal Procedure Act (big one) - also do 4943\n",
    "# merge_into_act_id = 4925 # Criminal Procedure Act (even bigger one)\n",
    "\n",
    "# first_act = nodes.match(\"Act\", act_id=to_merge_act_id).first()\n",
    "# first_act_name = first_act.get('act_sum')\n",
    "# print(first_act)\n",
    "# into_act = nodes.match(\"Act\", act_id=merge_into_act_id).first()\n",
    "# into_act_name = into_act.get('act_sum')\n",
    "# print(into_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this handles duplicates badly, so consider refactoring\n",
    "to_merge = [14375, 10835, 4916, 11238]\n",
    "merge_into = [14692, 10818, 4913, 11244]\n",
    "\n",
    "to_merge_nodes = nodes.match(\"Act\", act_id=IN(to_merge)).all()\n",
    "merge_into_nodes = nodes.match(\"Act\", act_id=IN(merge_into)).all()\n",
    "for to_merge_node, merge_into_node in zip(to_merge_nodes, merge_into_nodes):\n",
    "#     print(to_merge_node)\n",
    "#     print(merge_into_node)\n",
    "    merge_two_nodes(to_merge_node, merge_into_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_complete_acts = [{ \n",
    "    'removed_act_id': int(to_merge_node.get('act_id')),\n",
    "    'removed_act_name': to_merge_node.get('act_sum'),\n",
    "    'merged_into_act_id': int(merge_into_node.get('act_id')),\n",
    "    'merged_into_act_name': merge_into_node.get('act_sum')\n",
    "} for to_merge_node, merge_into_node in zip(to_merge_nodes, merge_into_nodes)]\n",
    "\n",
    "# print(newly_complete_acts)\n",
    "consolidated_acts += newly_complete_acts\n",
    "\n",
    "print(\"Adding to saved list: \", consolidated_acts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(consolidated_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('consolidated_acts.pkl', 'wb') as fp:\n",
    "    pickle.dump(consolidated_acts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-bench",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalgraph",
   "language": "python",
   "name": "legalgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
